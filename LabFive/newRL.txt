achieve more reward.
Without rewards there could be no values, and the only purpose of estimating values is to
Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary.
of how pleased or displeased we are that our environment is in a particular state.
and pain (if low), whereas values correspond to a more refined and farsighted judgment
could be true. To make a human analogy, rewards are somewhat like pleasure (if high)
because it is regularly followed by other states that yield high rewards. Or the reverse
example, a state might always yield a low immediate reward but still have a high value
account the states that are likely to follow and the rewards available in those states. For
environmental states, values indicate the long-term desirability of states after taking into
from that state. Whereas rewards determine the immediate, intrinsic desirability of
the total amount of reward an agent can expect to accumulate over the future, starting
function specifies what is good in the long run. Roughly speaking, the value of a state is
Whereas the reward signal indicates what is good in an immediate sense, a value
be stochastic functions of the state of the environment and the actions taken.
select some other action in that situation in the future. In general, reward signals may
selected by the policy is followed by low reward, then the policy may be changed to
by the agent. The reward signal is the primary basis for altering the policy; if an action
of pleasure or pain. They are the immediate and defining features of the problem faced
agent. In a biological system, we might think of rewards as analogous to the experiences
the long run. The reward signal thus defines what are the good and bad events for the
the reward. The agent’s sole objective is to maximize the total reward it receives over
step, the environment sends to the reinforcement learning agent a single number called
A reward signal defines the goal of a reinforcement learning problem. On each time
probabilities for each action.
is sucient to determine behavior. In general, policies may be stochastic, specifying
process. The policy is the core of a reinforcement learning agent in the sense that it alone
or lookup table, whereas in others it may involve extensive computation such as a search
stimulus–response rules or associations. In some cases the policy may be a simple function
when in those states. It corresponds to what in psychology would be called a set of
a policy is a mapping from perceived states of the environment to actions to be taken
A policy defines the learning agent’s way of behaving at a given time. Roughly speaking,
a model of the environment.
reinforcement learning system: a policy, a reward signal, a value function, and, optionally,
Beyond the agent and the environment, one can identify four main subelements of a
